{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dill\n",
    "import random\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voc(object):\n",
    "    '''Define the vocabulary (token) dict'''\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.idx2word = {}\n",
    "        self.word2idx = {}\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        '''add vocabulary to dict via a list of words'''\n",
    "        for word in sentence:\n",
    "            if word not in self.word2idx:\n",
    "                self.idx2word[len(self.word2idx)] = word\n",
    "                self.word2idx[word] = len(self.word2idx)\n",
    "\n",
    "# create voc set\n",
    "def create_str_token_mapping(df, vocabulary_file):\n",
    "    diag_voc = Voc()\n",
    "    med_voc = Voc()\n",
    "    pro_voc = Voc()\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        diag_voc.add_sentence(row[\"icd_code\"])\n",
    "        med_voc.add_sentence(row[\"ATC3\"])\n",
    "        pro_voc.add_sentence(row[\"pro_code\"])\n",
    "\n",
    "    dill.dump(\n",
    "        obj={\"diag_voc\": diag_voc, \"med_voc\": med_voc, \"pro_voc\": pro_voc},\n",
    "        file=open(vocabulary_file, \"wb\"),\n",
    "    )\n",
    "    return diag_voc, med_voc, pro_voc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0:\n",
    "\n",
    "Assign the STAY_ID to precription, procedure and diagnosis tables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:\n",
    "Preprocess the raw MIMIC-III data as the original medication recommendation works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"\"   # base folder\n",
    "\n",
    "## Some auxiliary info, such as DDI, ATC and ICD\n",
    "RXCUI2atc4_file = os.path.join(base_dir, \"./auxiliary/RXCUI2atc4.csv\")\n",
    "cid2atc6_file = os.path.join(base_dir, \"./auxiliary/drug-atc.csv\")\n",
    "ndc2RXCUI_file = os.path.join(base_dir, \"./auxiliary/ndc2RXCUI.txt\")\n",
    "ddi_file = os.path.join(base_dir, \"./auxiliary/drug-DDI.csv\")\n",
    "drugbankinfo = os.path.join(base_dir, \"./auxiliary/drugbank_drugs_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_file = os.path.join(base_dir, \"./hosp/prescriptions.csv\")\n",
    "diag_file = os.path.join(base_dir, \"./hosp/diagnoses_icd.csv\")\n",
    "procedure_file = (\n",
    "    os.path.join(base_dir, \"./hosp/procedures_icd.csv\")\n",
    ")\n",
    "\n",
    "# input auxiliary files\n",
    "med_structure_file = os.path.join(base_dir, \"./handled/atc32SMILES.pkl\")\n",
    "\n",
    "# output files\n",
    "ddi_adjacency_file = os.path.join(base_dir, \"./handled/ddi_A_final.pkl\")\n",
    "ehr_adjacency_file = os.path.join(base_dir, \"./handled/ehr_adj_final.pkl\")\n",
    "ehr_sequence_file = os.path.join(base_dir, \"./handled/records_final.pkl\")\n",
    "vocabulary_file = os.path.join(base_dir, \"./handled/voc_final.pkl\")\n",
    "ddi_mask_H_file = os.path.join(base_dir, \"./handled/ddi_mask_H.pkl\")\n",
    "atc3toSMILES_file = os.path.join(base_dir, \"./handled/atc3toSMILES.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './hosp/prescriptions.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# for med\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m med_pd \u001b[38;5;241m=\u001b[39m \u001b[43mmed_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmed_file\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# process the raw file\u001b[39;00m\n\u001b[1;32m      3\u001b[0m med_pd_lg2 \u001b[38;5;241m=\u001b[39m process_visit_lg2(med_pd)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)   \u001b[38;5;66;03m# remain the single-visit\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# med_pd_lg2 = process_visit_lg2(med_pd).reset_index(drop=True)   # filter out the patient has less 2 visits\u001b[39;00m\n",
      "File \u001b[0;32m~/FromWindows/data/mimic4/utils.py:28\u001b[0m, in \u001b[0;36mmed_process\u001b[0;34m(med_file)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmed_process\u001b[39m(med_file):\n\u001b[0;32m---> 28\u001b[0m     med_pd \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmed_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mndc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# remove redundant columnns\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     remove_columns \u001b[38;5;241m=\u001b[39m med_pd\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './hosp/prescriptions.csv'"
     ]
    }
   ],
   "source": [
    "# for med\n",
    "med_pd = med_process(med_file)  # process the raw file\n",
    "med_pd_lg2 = process_visit_lg2(med_pd).reset_index(drop=True)   # remain the single-visit\n",
    "# med_pd_lg2 = process_visit_lg2(med_pd).reset_index(drop=True)   # filter out the patient has less 2 visits\n",
    "med_pd = med_pd.merge(\n",
    "    med_pd_lg2[[\"subject_id\"]], on=\"subject_id\", how=\"inner\"\n",
    ").reset_index(drop=True)\n",
    "\n",
    "med_pd = codeMapping2atc4(med_pd, ndc2RXCUI_file, RXCUI2atc4_file)\n",
    "med_pd = filter_300_most_med(med_pd)\n",
    "\n",
    "# med to SMILES mapping\n",
    "atc3toDrug = ATC3toDrug(med_pd)\n",
    "druginfo = pd.read_csv(drugbankinfo)\n",
    "atc3toSMILES = atc3toSMILES(atc3toDrug, druginfo)\n",
    "dill.dump(atc3toSMILES, open(atc3toSMILES_file, \"wb\"))\n",
    "med_pd = med_pd[med_pd.ATC3.isin(atc3toSMILES.keys())]\n",
    "print(\"complete medication processing\")\n",
    "\n",
    "# for diagnosis\n",
    "diag_pd = diag_process(diag_file)\n",
    "\n",
    "print(\"complete diagnosis processing\")\n",
    "\n",
    "# for procedure\n",
    "pro_pd = procedure_process(procedure_file)\n",
    "# pro_pd = filter_1000_most_pro(pro_pd)\n",
    "\n",
    "print(\"complete procedure processing\")\n",
    "\n",
    "# combine\n",
    "data = combine_process(med_pd, diag_pd, pro_pd)\n",
    "print(\"complete combining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use a part of data\n",
    "np.random.seed(42)\n",
    "sample_patient = np.random.choice(data[\"subject_id\"].unique(), int(data[\"subject_id\"].unique().shape[0]*0.15), replace=False)\n",
    "data = data[data[\"subject_id\"].isin(sample_patient)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#patients  (8906,)\n",
      "#clinical events  23046\n",
      "#diagnosis  1998\n",
      "#med  125\n",
      "#procedure 1001\n",
      "#avg of diagnoses  8.414345222598282\n",
      "#avg of medicines  7.017399982643409\n",
      "#avg of procedures  2.1107784431137726\n",
      "#avg of vists  2.5876936896474287\n",
      "#max of diagnoses  220\n",
      "#max of medicines  72\n",
      "#max of procedures  49\n",
      "#max of visit  48\n"
     ]
    }
   ],
   "source": [
    "statistics(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtain voc\n",
      "obtain ehr sequence data\n",
      "obtain ddi adj matrix\n"
     ]
    }
   ],
   "source": [
    "# create vocab\n",
    "diag_voc, med_voc, pro_voc = create_str_token_mapping(data, vocabulary_file)\n",
    "print(\"obtain voc\")\n",
    "\n",
    "# create ehr sequence data\n",
    "records = create_patient_record(data, diag_voc, med_voc, pro_voc, ehr_sequence_file)\n",
    "print(\"obtain ehr sequence data\")\n",
    "\n",
    "# create ddi adj matrix\n",
    "ddi_adj = get_ddi_matrix(records, med_voc, ddi_file, cid2atc6_file, ehr_adjacency_file, ddi_adjacency_file)\n",
    "print(\"obtain ddi adj matrix\")\n",
    "\n",
    "# get ddi_mask_H\n",
    "ddi_mask_H = get_ddi_mask(atc3toSMILES, med_voc)\n",
    "dill.dump(ddi_mask_H, open(ddi_mask_H_file, \"wb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get side info\n",
    "Extract side information of patients from other csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_side(source_df, side_df, side_columns, aligh_column):\n",
    "\n",
    "    side_df = side_df[side_columns]\n",
    "    source_df = pd.merge(source_df, side_df, how=\"left\", on=aligh_column)\n",
    "\n",
    "    return source_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "admission = pd.read_csv(\"./hosp/admissions.csv\")\n",
    "data = get_side(data, admission, \n",
    "                [\"hadm_id\", \"insurance\", \"language\", \"admission_type\", \"marital_status\", \"race\"],\n",
    "                \"hadm_id\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(value=\"unknown\", inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Map ATC to drugname\n",
    "Resolve the mapping. In the original preprocessed data, the drug is represented by ATC code, but we need the drugname for LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RXCUI2atc4 = pd.read_csv(RXCUI2atc4_file)\n",
    "RXCUI2atc4[\"NDC\"] = RXCUI2atc4[\"NDC\"].map(lambda x: x.replace(\"-\", \"\"))\n",
    "with open(ndc2RXCUI_file, \"r\") as f:\n",
    "    ndc2RXCUI = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "RXCUI2ndc = dict(zip(ndc2RXCUI.values(), ndc2RXCUI.keys()))\n",
    "RXCUI2atc4[\"RXCUI\"] = RXCUI2atc4[\"RXCUI\"].astype(\"str\")\n",
    "RXCUI2atc4[\"NDC\"] = RXCUI2atc4[\"RXCUI\"].map(RXCUI2ndc)\n",
    "RXCUI2atc4.dropna(axis=0, how=\"any\", inplace=True)\n",
    "RXCUI2atc4.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32732, 5),\n",
       " YEAR       73\n",
       " MONTH      12\n",
       " NDC      2037\n",
       " RXCUI    2037\n",
       " ATC4      445\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RXCUI2atc4.shape, RXCUI2atc4.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32732, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RXCUI2atc4.drop_duplicates(inplace=True)\n",
    "RXCUI2atc4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>starttime</th>\n",
       "      <th>drug</th>\n",
       "      <th>ATC3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853</td>\n",
       "      <td>2180-05-07 00:00:00</td>\n",
       "      <td>Heparin</td>\n",
       "      <td>B01A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22841357</td>\n",
       "      <td>2180-06-26 22:00:00</td>\n",
       "      <td>Heparin</td>\n",
       "      <td>B01A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>25742920</td>\n",
       "      <td>2180-08-06 03:00:00</td>\n",
       "      <td>Heparin</td>\n",
       "      <td>B01A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034</td>\n",
       "      <td>2180-07-23 15:00:00</td>\n",
       "      <td>Heparin</td>\n",
       "      <td>B01A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000117</td>\n",
       "      <td>22927623</td>\n",
       "      <td>2181-11-15 13:00:00</td>\n",
       "      <td>Heparin</td>\n",
       "      <td>B01A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id           starttime     drug  ATC3\n",
       "0    10000032  22595853 2180-05-07 00:00:00  Heparin  B01A\n",
       "1    10000032  22841357 2180-06-26 22:00:00  Heparin  B01A\n",
       "2    10000032  25742920 2180-08-06 03:00:00  Heparin  B01A\n",
       "3    10000032  29079034 2180-07-23 15:00:00  Heparin  B01A\n",
       "4    10000117  22927623 2181-11-15 13:00:00  Heparin  B01A"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-1cd33d362751>:1: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  med_pd = pd.read_csv(med_file, dtype={\"ndc\": \"category\"})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['subject_id', 'hadm_id', 'pharmacy_id', 'poe_id', 'poe_seq',\n",
       "       'order_provider_id', 'starttime', 'stoptime', 'drug_type', 'drug',\n",
       "       'formulary_drug_cd', 'gsn', 'ndc', 'prod_strength', 'form_rx',\n",
       "       'dose_val_rx', 'dose_unit_rx', 'form_val_disp', 'form_unit_disp',\n",
       "       'doses_per_24_hrs', 'route'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_pd = pd.read_csv(med_file, dtype={\"ndc\": \"category\"})\n",
    "med_pd.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "RXCUI2atc4 = RXCUI2atc4.rename({\"NDC\": \"ndc\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_pd[\"ndc\"].astype(\"str\")\n",
    "med_pd = pd.merge(med_pd, RXCUI2atc4, how=\"left\", on=\"ndc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "atc2drug = pd.read_csv(\"./auxiliary/WHO ATC-DDD 2021-12-03.csv\")\n",
    "atc2drug[\"code_len\"] = atc2drug[\"atc_code\"].map(lambda x: len(x))\n",
    "atc2drug = atc2drug[atc2drug[\"code_len\"]==4]    # all levels are included. We only need the 4th level, i.e., ATC4\n",
    "atc2drug.rename(columns={\"atc_code\": \"ATC4\"}, inplace=True)\n",
    "atc2drug.drop(columns=[\"ddd\", \"uom\", \"adm_r\", \"note\", \"code_len\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATC4</th>\n",
       "      <th>atc_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A01A</td>\n",
       "      <td>STOMATOLOGICAL PREPARATIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>A02A</td>\n",
       "      <td>ANTACIDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>A02B</td>\n",
       "      <td>DRUGS FOR PEPTIC ULCER AND GASTRO-OESOPHAGEAL ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ATC4                                           atc_name\n",
       "2   A01A                        STOMATOLOGICAL PREPARATIONS\n",
       "46  A02A                                           ANTACIDS\n",
       "79  A02B  DRUGS FOR PEPTIC ULCER AND GASTRO-OESOPHAGEAL ..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atc2drug.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "RXCUI2atc4[\"ATC4\"] = RXCUI2atc4[\"ATC4\"].map(lambda x: x[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all atc code in original data can be mapped to drugname by atc2drug.\n",
    "# means that we use the same data as the traditional medication recommendation models.\n",
    "pd.merge(RXCUI2atc4, atc2drug, on=\"ATC4\", how=\"left\")[\"atc_name\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "atc2drug[\"atc_name\"] = atc2drug[\"atc_name\"].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the atc2drug and drug2atc mapping dict\n",
    "atc2drug_dict = dict(zip(atc2drug[\"ATC4\"].values, atc2drug[\"atc_name\"].values))\n",
    "drug2atc_dict = dict(zip(atc2drug[\"atc_name\"].values, atc2drug[\"ATC4\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump({\"atc2drug\": atc2drug_dict, \"drug2atc\": drug2atc_dict}, open(\"./handled/atc2drug.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the diagnosis and procedure mapping dict, which both use the ICD. these mappings are in raw MIMIC dataset\n",
    "icd2diag = pd.read_csv(\"./hosp/d_icd_diagnoses.csv\")\n",
    "icd2diag_dict = dict(zip(icd2diag[\"icd_code\"].astype(str).values, icd2diag[\"long_title\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd2proc = pd.read_csv(\"./hosp/d_icd_procedures.csv\")\n",
    "icd2proc_dict = dict(zip(icd2proc[\"icd_code\"].astype(str).values, icd2proc[\"long_title\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(code_list, decoder):\n",
    "    # decode a list of code into corresponding names\n",
    "    miss_match = 0\n",
    "    target_list = []\n",
    "    for code in code_list:\n",
    "        try:\n",
    "            target_list.append(decoder[code])\n",
    "        except:\n",
    "            miss_match += 1\n",
    "    \n",
    "    #print(miss_match)\n",
    "\n",
    "    return target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"drug\"] = data[\"ATC3\"].map(lambda x: decode(x, atc2drug_dict))\n",
    "data[\"diagnosis\"] = data[\"icd_code\"].map(lambda x: decode(x, icd2diag_dict))\n",
    "data[\"procedure\"] = data[\"pro_code\"].map(lambda x: decode(x, icd2proc_dict))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some miss matches occurs in diagnosis and procedures, but no for drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8669']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[1][\"pro_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_tokenization(df, profile_columns):\n",
    "    prof_dict = {\"word2idx\":{}, \"idx2word\": {}}\n",
    "    for prof in profile_columns:\n",
    "        prof_dict[\"idx2word\"][prof] = dict(zip(range(df[prof].nunique()), df[prof].unique()))\n",
    "        prof_dict[\"word2idx\"][prof] = dict(zip(df[prof].unique(), range(df[prof].nunique())))\n",
    "    return prof_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_dict = profile_tokenization(data, [\"insurance\", \"language\", \"admission_type\", \"marital_status\", \"race\"])\n",
    "json.dump(profile_dict, open(\"./handled/profile_dict.json\", \"w\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Construct Prompt\n",
    "Design the prompt templates and construct the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt templates\n",
    "main_template = \"The patient has <VISIT_NUM> times ICU visits. \\n <HISTORY> In this visit, he has diagnosis: <DIAGNOSIS>; procedures: <PROCEDURE>. Then, the patient should be prescribed: \"\n",
    "hist_template = \"In <VISIT_NO> visit, the patient had diagnosis: <DIAGNOSIS>; procedures: <PROCEDURE>. The patient was prescribed drugs: <MEDICATION>. \\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some patient's profiles\n",
    "# main_template = \"The patient's insurance type is <INSU>, language is <LANG>, admission type is <ADMTYPE>, marital status is <MARITAL>, race is <RACE>. The patient has <VISIT_NUM> times ICU visits. \\n <HISTORY> In this visit, he has diagnosis: <DIAGNOSIS>; procedures: <PROCEDURE>. Then, the patient should be prescribed: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_str(str_list):\n",
    "    # concat a list of drug / diagnosis / procedures\n",
    "    target_str = \"\"\n",
    "    for meta_str in str_list:\n",
    "        target_str = target_str + meta_str + \", \"\n",
    "    target_str = target_str[:-2]    # remove the last comma\n",
    "\n",
    "    return target_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_data = []\n",
    "\n",
    "for subject_id in data[\"subject_id\"].unique():\n",
    "    item_df = data[data[\"subject_id\"] == subject_id]\n",
    "    visit_num = item_df.shape[0] - 1\n",
    "    patient = []\n",
    "\n",
    "    profile = item_df.iloc[0]\n",
    "    patient_str = main_template.replace(\"<INSU>\", profile[\"insurance\"].lower())\\\n",
    "                               .replace(\"<LANG>\", profile[\"language\"].lower())\\\n",
    "                               .replace(\"<ADMTYPE>\", profile[\"admission_type\"].lower())\\\n",
    "                               .replace(\"<MARITAL>\", profile[\"marital_status\"].lower())\\\n",
    "                               .replace(\"<RACE>\", profile[\"race\"].lower())\n",
    "\n",
    "    patient_profile = {\"insurance\": profile[\"insurance\"], \"language\": profile[\"language\"],\n",
    "                       \"admission_type\": profile[\"admission_type\"], \"marital_status\": profile[\"marital_status\"],\n",
    "                       \"race\": profile[\"race\"]}\n",
    "\n",
    "    # get each historical visit string\n",
    "    for visit_no, (_, row) in enumerate(item_df.iterrows()):\n",
    "        drug, diag, proc = concat_str(row[\"drug\"]), concat_str(row[\"diagnosis\"]), concat_str(row[\"procedure\"])\n",
    "        patient.append(hist_template.replace(\"<VISIT_NO>\", str(visit_no+1))\\\n",
    "                                    .replace(\"<DIGNOSIS>\", diag)\\\n",
    "                                    .replace(\"<PROCEDURE>\", proc)\\\n",
    "                                    .replace(\"<MEDICATION>\", drug))\n",
    "    patient.pop()   # remove the ground truth record\n",
    "\n",
    "    # filter out the patients with more than 3 times visits\n",
    "    if len(patient) > 3:\n",
    "        patient = patient[-3:]\n",
    "\n",
    "    # concat all historical visit strings and get hist strings\n",
    "    hist_str = \"\"\n",
    "    for meta_hist in patient:\n",
    "        hist_str += meta_hist\n",
    "    \n",
    "    patient_str = patient_str.replace(\"<VISIT_NUM>\", str(visit_num))\\\n",
    "                             .replace(\"<HISTORY>\", hist_str)\\\n",
    "                             .replace(\"<DIAGNOSIS>\", diag)\\\n",
    "                             .replace(\"<PROCEDURE>\", proc)\n",
    "    \n",
    "    drug_code = [str(x) for x in row[\"ATC3\"]]\n",
    "\n",
    "    hist = {\"diagnosis\": [], \"procedure\": [], \"medication\": []}\n",
    "    for _, row in item_df.iterrows():\n",
    "        hist[\"diagnosis\"].append([str(x) for x in row[\"icd_code\"]])\n",
    "        hist[\"procedure\"].append([str(x) for x in row[\"pro_code\"]])\n",
    "        hist[\"medication\"].append([str(x) for x in row[\"ATC3\"]])\n",
    "        \n",
    "    llm_data.append({\"input\": patient_str, \"target\": drug, \n",
    "                     \"subject_id\": int(subject_id), \"drug_code\": drug_code,\n",
    "                     \"records\": hist, \"profile\": patient_profile})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./handled/\"\n",
    "\n",
    "def read_data(data_path):\n",
    "    '''read data from jsonlines file'''\n",
    "    data = []\n",
    "\n",
    "    with jsonlines.open(file_path + data_path, \"r\") as f:\n",
    "        for meta_data in f:\n",
    "            data.append(meta_data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_data(data_path, data):\n",
    "    '''write all_data list to a new jsonl'''\n",
    "    with jsonlines.open(file_path + data_path, \"w\") as w:\n",
    "        for meta_data in data:\n",
    "            w.write(meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset: 8:1:1\n",
    "train_split = int(len(llm_data) * 0.8)\n",
    "val_split = int(len(llm_data) * 0.1)\n",
    "train = llm_data[:train_split]\n",
    "val = llm_data[train_split:train_split+val_split]\n",
    "test = llm_data[train_split+val_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"train_0104.json\", train)\n",
    "save_data(\"val_0104.json\", val)\n",
    "save_data(\"test_0104.json\", test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
